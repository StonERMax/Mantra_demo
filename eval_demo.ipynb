{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://github.com/CyanideBoy/Mantra_demo/blob/master/eval_demo.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf Mantra_demo\n",
    "!git clone https://github.com/CyanideBoy/Mantra_demo.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import sys\n",
    "import numpy as np \n",
    "import cv2 as cv2\n",
    "import lmdb\n",
    "import keras\n",
    "from matplotlib import pyplot\n",
    "np.set_printoptions( 3, suppress = True )\n",
    "from tensorflow.python.client import device_lib\n",
    "from tf_multi_gpu import make_parallel \n",
    "print device_lib.list_local_devices()\n",
    "from keras.utils import to_categorical\n",
    "import modelCore\n",
    "\n",
    "print cv2.__version__\n",
    "fontsize = 24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "debug = False   # Setting it True gives performance similar to what paper reports\n",
    "idx = 1         # Model id (0-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Model and Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import sys\n",
    "#sys.path.insert( 0, 'sequence/')\n",
    "\n",
    "manTraNet_root = './'\n",
    "manTraNet_modelDir = os.path.join( manTraNet_root, 'pretrained_weights' )\n",
    "\n",
    "def get_single_gpu_model(idx) :\n",
    "    \n",
    "    mantra_model = modelCore.load_pretrain_model_by_index( idx, manTraNet_modelDir )\n",
    "    print mantra_model.summary(line_length=120)\n",
    "    \n",
    "    return mantra_model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score, f1_score\n",
    "import tensorflow as tf\n",
    "\n",
    "def np_F1( y_true, y_pred ) :\n",
    "    score = []\n",
    "    for yy_true, yy_pred in zip( y_true, y_pred ) :\n",
    "        this = f1_score( (yy_true>.5).astype('int').ravel(), (yy_pred>.5).astype('int').ravel() )\n",
    "        that = f1_score( (yy_true>.5).astype('int').ravel(), (1-yy_pred>.5).astype('int').ravel() )\n",
    "        score.append( max( this, that ) )\n",
    "    return np.mean( score ).astype('float32')\n",
    "\n",
    "\n",
    "def F1( y_true, y_pred ) :\n",
    "    return tf.py_func( np_F1, [y_true, y_pred], 'float32')\n",
    "\n",
    "def np_auc( y_true, y_pred ) :\n",
    "    score = []\n",
    "\n",
    "    for yy_true, yy_pred in zip( y_true, y_pred ) :\n",
    "        this = roc_auc_score( (yy_true>.5).astype('int').ravel(), yy_pred.ravel() )\n",
    "        that = roc_auc_score( (yy_true>.5).astype('int').ravel(), 1-yy_pred.ravel() )\n",
    "        score.append( max( this, that ) )\n",
    "    return np.mean( score ).astype('float32')\n",
    "\n",
    "def auroc(y_true, y_pred):\n",
    "    return tf.py_func( np_auc, [y_true, y_pred], 'float32')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_coverage_dataset() :\n",
    "    input_image_file_list = 'COVERAGE_image_new.list'\n",
    "    with open( input_image_file_list, 'r') as IN :\n",
    "        input_files = [ line.strip() for line in IN.readlines() ]\n",
    "    print \"INFO: successfully load\", len( input_files ), \"input files\"\n",
    "\n",
    "    def get_input_ID( input_file ) :\n",
    "        bname = os.path.basename( input_file )\n",
    "        return bname.rsplit('.')[0]\n",
    "\n",
    "    def get_mask_file_from_ID( sample_id ) :\n",
    "        return os.path.join('VERAGE/mask/', '{}forged.tif'.format(sample_id[:-1]) ) \n",
    "\n",
    "    def preprocess( input_image, input_mask ) :\n",
    "        x = np.expand_dims( input_image, axis=0 ).astype('float32')/255. * 2 - 1\n",
    "        y = np.expand_dims( np.expand_dims( input_mask, axis=0 ), axis=-1 )/255.\n",
    "        return x, y\n",
    "\n",
    "    raw_lut = dict( zip( [ get_input_ID(f) for f in input_files ], input_files) )\n",
    "\n",
    "    paired_results = []\n",
    "    for key in raw_lut.keys() : \n",
    "        raw_file = raw_lut[key]\n",
    "        mask_file = get_mask_file_from_ID(key)\n",
    "        \n",
    "        r = cv2.imread( raw_file, 1 )[...,::-1]\n",
    "        m = cv2.imread( mask_file, 0)\n",
    "        if r.shape[:2] != m.shape[:2] :\n",
    "            continue\n",
    "        \n",
    "        raw_mask_dec = ( raw_file, mask_file )\n",
    "        paired_results.append( raw_mask_dec )\n",
    "\n",
    "    print len(paired_results)\n",
    "    return paired_results, len(paired_results), preprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate model performance for each dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_evaluation_data_generator( paired_results, preprocess ) :\n",
    "    for raw_file, mask_file in paired_results :\n",
    "        r = cv2.imread( raw_file, 1 )[...,::-1]\n",
    "        m = cv2.imread( mask_file, 0)\n",
    "        if r.shape[:2] != m.shape[:2] :\n",
    "            print \"INFO: find unmatched\", raw_file, mask_file, \", skip\"\n",
    "            continue\n",
    "        x, y = preprocess( r, m )\n",
    "        yield x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_single_gpu_model(idx)\n",
    "model.compile( optimizer='sgd',\n",
    "               loss ='binary_crossentropy',\n",
    "               metrics=['accuracy', F1, auroc], )\n",
    "\n",
    "\n",
    "from prettytable import PrettyTable\n",
    "import os\n",
    "import json\n",
    "\n",
    "table = PrettyTable()\n",
    "table.field_names = ['Dataset', 'Loss', 'Acc/AUC', 'F1' ]\n",
    "\n",
    "mega_lut = dict()\n",
    "\n",
    "for prepare_dataset, name in zip( [ prepare_coverage_dataset ],\n",
    "                                  ['COVERAGE'] ) :\n",
    "    input_pairs, L, preprocess = prepare_dataset()\n",
    "    # create data generator\n",
    "    datagen = create_evaluation_data_generator( input_pairs, preprocess ) \n",
    "    res = model.evaluate_generator( datagen, L if not debug else 1, verbose=1 )\n",
    "    # print \n",
    "    lut = dict( zip( model.metrics_names, res ) )\n",
    "    print name, lut\n",
    "    mega_lut[name] = lut\n",
    "    # update\n",
    "    table.add_row( [name] + [ \"{:.4f}\".format(lut[key]) for key in ['loss', 'auroc', 'F1'] ] )\n",
    "\n",
    "print (table)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
